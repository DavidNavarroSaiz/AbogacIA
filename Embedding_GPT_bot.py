"""
Class representing a chatbot that utilizes word embeddings for context and interacts with GPT-3.5 Turbo for answering user questions.
"""
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from dotenv import load_dotenv
import os
from langchain_community.llms import OpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain_community.callbacks.manager import get_openai_callback
from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

from langchain_mongodb import MongoDBChatMessageHistory


import openai
import time

class EmbeddingChatBot():
   
    def __init__(self,session_id):
        """
        Initialize the EmbeddingChatBot instance.

        Args:
            model_name (str): The name of the word embedding model to use. Default is "openai".
        """
        load_dotenv()
        self.docs = []
        self.question = ""
        self.doc_scores = []
        self.context = []
        self.gpt_answer = ""
        self.total_cost = 0
        self.session_id = session_id
        self.ef = OpenAIEmbeddings()
        openai.api_key = os.getenv("OPENAI_API_KEY")
               
        self.vectordb = Chroma(persist_directory=f"./abogacia_data", embedding_function=self.ef)
        connection_string = "mongodb://localhost:27017"


        self.message_history = MongoDBChatMessageHistory(
            connection_string=connection_string, session_id=self.session_id
        )
        

        print("There are",  self.vectordb._collection.count(), "in the collection")
        last_memory_messages = 2
        self.memory = ConversationBufferWindowMemory(k=last_memory_messages, memory_key="chat_history", input_key='question', output_key='answer', return_messages=True,chat_memory=self.message_history)
        

        
    def ask_embedding_bot(self,user_question):
        """
        Process user's question and generate a response.

        Args:
            user_question (str): The user's input question.

        Returns:
            str: The response generated by the chatbot.
        """
        self.user_question = user_question
        context,sources = self.similarity_search()
        self.gpt_answer = self.GPT_answer_from_embeddings(context)   
        self.memory.save_context({"question": self.user_question}, {"answer": self.gpt_answer })
        return self.gpt_answer

        # Define a function to filter results by relevance score
    def filter_results_by_score(self,results, threshold= 0.7):
        """
        Filters a list of documents by a relevance threshold.

        Args:
            results (list): A list of (doc, score) pairs.
            threshold (float): The minimum score required to keep a result.

        Returns:
            list: A filtered list of (doc, score) pairs.
        """
        self.doc_scores = [doc[1] for doc in results]

        return [doc for doc, score in results if score >= threshold]
    
    def similarity_search(self,threshold_filter_results = 0.6,number_docs = 6 ):
        """
        Perform similarity search for extracting relevant documents of the vector database and give context to the question.

        Args:
            threshold_filter_results (float): The relevance threshold for filtering results.
            number_docs (int): The number of documents to retrieve in the similarity search.
        """
        self.docs = self.vectordb.similarity_search_with_relevance_scores(self.user_question,k = number_docs)  
        sources = [doc[0].metadata.get('source') for doc in self.docs]
        print('Sources: \n ')
        for source in sources:
            print(f"Source: {source}")
        
        self.context  = self.filter_results_by_score(self.docs, threshold_filter_results)
        return self.context,sources

    def GPT_answer_from_embeddings(self,context, model = "gpt-3.5-turbo-0125"):
        """
        Generate a response using GPT-3.5 Turbo.

        Args:
            model (str): The GPT-3.5 Turbo model name.
            answer_temp (float): The temperature for response generation.
            answer_max_tokens (int): The maximum number of tokens in the generated response.
            
        Returns:
            str: The response generated by GPT-3.5 Turbo.
        """
        
        template = ("""
                                        
                    
                    - You give recommendations, build documents, and continuously ask how you can help.
                    - provide complete informative answers, Focus on providing helpful and relevant information,
                    - Always Answer the Question in the same language as the user question.
                    - you return the helpful answer directly
                    - if you are asked for a document, letter, email or similar, please return the document template with all the required information.
                    
                    use the context as reference that may help in the juridical case, however if you dont consider it useful information still try to help the person
                    remember that the new user question can be related with the chat history.

                    Context to answer question:\n{context}

                    Chat History:\n{formatted_chat_history}

                    Answer the User question:\n{question}
                    """)

        # Extract chat history from the memory variable
        chat_history = self.memory.load_memory_variables({}) 
        # Format chat history for display in the template
        formatted_chat_history = ""
        for message in chat_history['chat_history']:
            if isinstance(message, HumanMessage):
                formatted_chat_history += f"Human: {message.content}\n"
            elif isinstance(message, AIMessage):
                formatted_chat_history += f"AI System: {message.content}\n"

        print(formatted_chat_history)


        # Introduce formatted chat history into the template
        formatted_template = template.format(
            context=context,  # Provide the context variable
            formatted_chat_history=formatted_chat_history,
            question=self.user_question
        )

        print("formatted_template",formatted_template)

        prompt = [
    {"role": "system", "content": "You are a lawyer expert assistant that helps to solve, instruct and assist to a lawyer in different juridical cases "},
    {"role": "user", "content": formatted_template}
]
    # Send the conversation to GPT-3.5 Turbo
        response = openai.chat.completions.create(
            model=model,
            messages=prompt,
        )
        # cost = response.get('usage')
        # self.total_cost += cost.total_tokens
        # print('cost',cost)
        # print('total cost',self.total_cost)
        # Extract the assistant's reply from the response
        gpt_answer = response.choices[0].message.content
        return gpt_answer

    
if __name__ == "__main__":
    session_id = 'test_session_1'
    chatbot = EmbeddingChatBot(session_id)    
    questions = preguntas = [
    "Solicita asesoría sobre los derechos y obligaciones en casos de abandono de menores.",
    "¿Cómo proceder legalmente ante un caso de abandono de bienes por parte de un cónyuge?",
    "Necesito orientación sobre el proceso de divorcio y los pasos legales a seguir.",
    "Genera una carta para solicitar la custodia de un menor ante el juzgado.",
    "Ayúdame a redactar un documento de conciliación en un proceso de divorcio.",
    "Quiero presentar una petición ante el juzgado de familia, ¿qué información necesito incluir?",
    "¿Cuáles son los requisitos legales para establecer una pensión alimenticia?",
    "Proporciona orientación sobre cómo responder a una solicitud de paternidad.",
    "Necesito redactar una carta de notificación sobre el incumplimiento de visitas a menores.",
    "¿Qué documentos son necesarios para iniciar un proceso de adopción?",
    "¿Cómo solicitar una modificación de medidas en un proceso de divorcio?",
    "Ayúdame a redactar una queja formal ante la entidad reguladora.",
    "Genera un documento para solicitar la revisión de una sentencia judicial.",
    "Solicita información sobre los derechos de visita en casos de custodia compartida.",
    "¿Qué pasos debo seguir para realizar una separación de bienes?",
    "Ayúdame a redactar una carta de reclamación por incumplimiento de contrato.",
    "Necesito orientación sobre cómo presentar una demanda por violencia intrafamiliar.",
    "¿Qué información debo incluir en una solicitud de medidas cautelares?",
    "Genera un documento para solicitar la liquidación de bienes gananciales.",
    "Solicita asistencia para redactar una carta de autorización para representación legal."
]

    for i, question in enumerate(questions, 1):
        start_time = time.time()
        answer = chatbot.ask_embedding_bot(question)
        end_time = time.time()
        
        print(f"Question {i} - Time taken: {end_time - start_time} seconds")
        print("answer: \n", answer)
        print("\n" + "="*50 + "\n")  # Separator for better readability
    # print(result)
    # print(Embedding.doc_scores)
 
